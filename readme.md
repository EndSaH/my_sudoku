## Todolist

- 使用命令行参数指导

## 整体介绍

我们搜索了网络已有项目，选择 `https://github.com/Kyubyong/sudoku` 的代码作为重要参考。

在我们看来，该项目缺点有以下几个：

- 项目过于久远，甚至是 python2 + tensorflow 1.1.0，运行起来遇到众多麻烦；
- 没有对于使用 CNN 解决数独问题的思路与原理详细描述；
- 没有对超参数与架构的调整进行探讨；
- 训练数据集随机生成，没有经过筛选；
- 测试仅以准确率 (Acc) 作为参考标准，但没有考虑多解情况。

首先我们大幅调整了代码以适配 python3+pytorch，并在改动后使用原本的数据集 `sudoku.csv` 和测试集 `test.csv` 进行了训练与测试，得到了 `results/output_origin.txt`，大致与原模型的 Acc 率相当。

并在下面对尝试对训练数据集、架构、超参数、测试等进行改进。

我们在 `test.py` 中添加了完美率 (Perfect Rate, PR) 的测试，使用独立代码逻辑检测模型的输出是否符合数独逻辑，并在我们组织的新测试集上运行测试，得到了 `output_orogin_model_with_new_test.txt`，Acc, PR 分别为 0.6790, 0.2504。

目录下的 `data` 和 `logdir` 未上传，`data` 用于存放训练与测试数据，`logdir` 用于存放训练好的模型。

`data` 内部的文件如下：

- `full_test_set_10000.csv`
- `full_train_set_2990000.csv`
- `sudoku.csv`
- `test.csv`

前二者是我们自行搜集整理的数据集，后二者是原项目的数据集。

在这之后，我们对模型训练流程作如下调整：

- 使用了从可靠网站上下载并自行筛选整理的非随机高质量数据集 `full_train_set_2990000.csv` 用于训练；
- 根据经验，我们认为添加 ResNet 设计会显著提高性能，我们考虑添加 `ResBlock` 后将层数从 10 层增加到 20 层；
- 为适配现代显卡性能，将 `batch_size` 调整为 `1024`。


## 环境配置

标准测试环境为 Ubuntu 22.04 LTS, RTX 4090, cuda 12.8, pytorch 2.9.1。

需要安装 `tqdm + pytorch`:

```bash
# 如可以，最好新建一个单独环境来运行代码
# conda create -n sudoku python=3.12
# conda activate sudoku

# 此外，可能需要检查本地的 cuda 版本来安装对应的 pytorch
# 此处需要自行参考 pytorch 的官网文档。标准测试环境是 RTX 4090
pip3 install torch torchvision
pip3 install tqdm
```

## 运行指令

调整 `hyperparams.py` 指定训练集和测试集，依次执行 `train.py` 和 `test.py`。

## gemini 对于代码结构的分析

这五份代码构建了一个基于 **全卷积神经网络（Fully Convolutional Network, FCN）** 的深度学习模型，用于解决数独问题。

与其说是在“解题”（像人类那样通过逻辑推理），这个模型更像是在做**图像语义分割**或**像素级分类**。它将数独网格视为一张  的单通道图片，并试图将每个“像素”（格子）分类为 1-9 中的某一个数字。

以下是基于代码的详细分析，分为**整体解题思路**和**架构设计**两部分。

#### 1. 问题建模

模型不包含任何显式的数独规则（如“每行不能重复”、“宫内不能重复”）。它完全依赖神经网络强大的特征提取能力，从大量数据中隐式地学习这些约束。

* **输入 (Input):** 一个  的矩阵。
* 已知数字保持原值 (1-9)。
* **空白格用 0 表示**。
* 数据来源见 `data_load.py`，它将字符串解析为 Numpy 数组，再转为 Tensor。


* **输出 (Output):** 一个大小为  的张量。
* 最后维度的 9 代表数字 1 到 9 的**概率分布（Logits）**。
* 例如，位置  上的长度为 9 的向量，代表该格子填入 1-9 各个数字的可能性。

#### 2. 训练策略 (Masked Loss)

在 `train.py` 中，有一个非常关键的设计，即**只对空白格计算损失**。

* **掩码生成:** `istarget = (x == 0).float()`
* 这段代码创建了一个掩码，标记了哪些位置原本是空白（需要填写的）。


* **损失计算:**
```python
loss_map = criterion(logits, y - 1)
loss = (loss_map * istarget).sum() / (istarget.sum() + 1e-8)

```


* 模型会对所有 81 个格子都输出预测，但我们只关心原来是 0 的那些格子的预测是否正确。
* **注意标签偏移:** 代码中 `y - 1` 是因为数独数字是 1-9，而 PyTorch 的 `CrossEntropyLoss` 对应的类别索引是 0-8。



---

### 二、 CNN 架构设计详细分析

模型的架构定义在 `model.py` 和 `modules.py` 中，超参数在 `hyperparams.py` 中。这是一个典型的 **Deep & Wide** 全卷积结构。

#### 1. 架构概览 (SudokuNet)

这个网络没有池化层（Pooling），也没有全连接层（Linear）。它通过保持空间分辨率不变，让每一层的信息都对应原始的数独网格位置。

* **输入层:**
* 原始输入  被 unsqueeze 为 。
* 这里 `1` 是输入通道数（灰度图）。


* **特征提取器 (Encoder / Backbone):**
* 由 `hp.num_blocks` (10层) 个 `ConvBlock` 堆叠而成。
* **卷积核:**  (`hp.filter_size`)。
* **通道数:** 每一层都维持 **512 个通道** (`hp.num_filters`)。这是一个非常宽的网络，意味着每个格子在每一层都有一个 512 维的向量来表示其上下文特征。
* **Padding:** 使用 "SAME" padding（在 `ConvBlock` 中手动计算）。
* **目的:** 保证经过卷积后，输出的长宽依然是 。这对数独至关重要，因为每个格子的位置信息必须保留。




* **分类头 (Logits Layer):**
* `nn.Conv2d(512, 9, kernel_size=1)`
* 这是一个  卷积层。它的作用是将 512 维的高维特征压缩为 9 维，对应 9 个候选数字的得分。



#### 2. 核心模块 (ConvBlock)

在 `modules.py` 中定义：

* **结构:** Conv2d -> BatchNorm -> ReLU。
* **归一化:** 代码默认使用了 `norm_type="bn"` (BatchNorm)。这有助于训练深层网络（10层）时的梯度传播和收敛速度。
* **感受野 (Receptive Field) 的作用:**
* 第一层卷积只能看到  的局部区域（一个九宫格）。
* 随着层数加深（堆叠 10 层  卷积），每个格子的感受野会逐渐扩大，最终覆盖整个  面板。
* **原理:** 第 10 层的某个格子，能够“看见”并整合全图的信息，从而判断出根据行、列、宫的约束，当前应该填什么数字。



---

### 三、 为什么这样设计？(关键点总结)

1. **全卷积 (FCN) 保持空间结构:**
数独是一个强位置相关的游戏。 的数字直接影响  的数字。传统的 CNN 会用池化层降低分辨率，但这在这里是不可接受的。必须全程保持 。
2. **深层堆叠 (10 Blocks):**
为了让左上角的格子“知道”右下角的数字，网络必须足够深，以便感受野覆盖全图。如果层数太少，模型可能只能学到局部的“九宫格”规则，而学不到长距离的“行/列”规则。
3. **高通道数 (512 Channels):**
数独的逻辑非常复杂。相对于图像识别提取边缘、纹理，数独需要提取的是**数字的组合逻辑**。512 个通道提供了巨大的容量，让神经网络在每个格子上存储复杂的上下文状态（例如：“这行已经有了1, 5, 7，这列有了 2, 8...”）。
4. **端到端的像素级分类:**
这种设计避免了复杂的搜索算法（如回溯法）。模型直接根据输入图像输出结果，推理速度非常快（只需一次 Forward Pass）。

### 总结

这套代码实现了一个**基于全局感受野的像素级分类器**。它不教计算机“数独规则”，而是通过喂给它大量的 `(题目, 答案)` 对，让一个深层、宽通道的 CNN 自动学会数独背后复杂的空间约束逻辑。